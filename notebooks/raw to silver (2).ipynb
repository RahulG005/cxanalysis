{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e807068-f697-4832-9355-88cca3e477ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scope = 'cxkeyvault'\n",
    "sas_token_key = 'cxdatalake'\n",
    "storage_account = 'customeranalysisdatalake'\n",
    "container = 'raw'\n",
    "container2 = 'silver'\n",
    "\n",
    "sas_token = dbutils.secrets.get(scope=scope, key=sas_token_key)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18cdc4f9-3ab1-4b04-8426-f270ca308a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68261606-fb5b-4752-9065-a29aae724891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#all paths\n",
    "\n",
    "accounts_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/accounts/\")\n",
    "customers_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/customers/\")\n",
    "loan_payments_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/loan payments/\")\n",
    "loans_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/loans/\")\n",
    "transactions_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/transactions/\")\n",
    "\n",
    "accounts_raw_path = (f\"abfss://{container}@{storage_account}.dfs.core.windows.net/accounts/\")\n",
    "customers_raw_path = (f\"abfss://{container}@{storage_account}.dfs.core.windows.net/customers/\")\n",
    "loan_payments_raw_path = (f\"abfss://{container}@{storage_account}.dfs.core.windows.net/loan payments/\")\n",
    "loans_raw_path = (f\"abfss://{container}@{storage_account}.dfs.core.windows.net/loans/\")\n",
    "transactions_raw_path = (f\"abfss://{container}@{storage_account}.dfs.core.windows.net/transactions/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0b825c-f144-48ab-8d8e-dd0e456ddb66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Accounts delta lake empty table\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType\n",
    "# schema = StructType([ StructField(\"account_id\", IntegerType(), True),\n",
    "#                      StructField(\"customer_id\", IntegerType(), True), \n",
    "#                      StructField(\"account_type\", StringType(), True), \n",
    "#                      StructField(\"balance\", DecimalType(10,2), True)\n",
    "#                      ])\n",
    "\n",
    "# accounts_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/accounts/\")\n",
    "\n",
    "# # Create an empty DataFrame with the schema \n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.format(\"delta\").save(accounts_deltatable_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b71630-8b19-4d3f-a971-4b9248b60b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #customers delta lake empty table\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType\n",
    "# schema = StructType([\n",
    "#                     StructField(\"customer_id\", IntegerType(), True), \n",
    "#                     StructField(\"first_name\", StringType(), True),\n",
    "#                     StructField(\"last_name\", StringType(), True),\n",
    "#                     StructField(\"address\", StringType(), True),\n",
    "#                     StructField(\"city\", StringType(), True),\n",
    "#                     StructField(\"state\", StringType(), True),\n",
    "#                     StructField(\"zip\", StringType(), True),\n",
    "#                      ])\n",
    "\n",
    "# customers_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/customers/\")\n",
    "\n",
    "# # Create an empty DataFrame with the schema \n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.format(\"delta\").save(customers_deltatable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee211317-9d0f-44a3-9fcb-c0f590a2ef8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #loan payments delta lake empty table\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "# schema = StructType([\n",
    "#                     StructField(\"payment_id\", IntegerType(), True),\n",
    "#                     StructField(\"loan_id\", IntegerType(), True),\n",
    "#                     StructField(\"payment_date\", DateType(), True),\n",
    "#                     StructField(\"payment_amount\", IntegerType(), True),\n",
    "#                      ])\n",
    "\n",
    "# loan_payments_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/loan payments/\")\n",
    "\n",
    "# # Create an empty DataFrame with the schema \n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.format(\"delta\").save(loan_payments_deltatable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84c5d92-4d6c-4047-9189-68c095e8f40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #loans delta lake empty table\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "# schema = StructType([\n",
    "#                     StructField(\"loan_id\", IntegerType(), True),\n",
    "#                     StructField(\"customer_id\", IntegerType(), True),\n",
    "#                     StructField(\"loan_amount\", DecimalType(10,2), True),\n",
    "#                     StructField(\"interest_rate\", DecimalType(4,2), True),\n",
    "#                     StructField(\"loan_term\", IntegerType(), True),\n",
    "#                      ])\n",
    "\n",
    "# loans_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/loans/\")\n",
    "\n",
    "# # Create an empty DataFrame with the schema \n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.format(\"delta\").save(loans_deltatable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6e0884-4287-4522-999e-027b77706c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #transactions delta lake empty table\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "# schema = StructType([\n",
    "#                     StructField(\"transaction_id\", IntegerType(), True),\n",
    "#                     StructField(\"account_id\", IntegerType(), True),\n",
    "#                     StructField(\"transaction_date\", DateType(), True),\n",
    "#                     StructField(\"transaction_amount\", DecimalType(10,2), True),\n",
    "#                     StructField(\"transaction_type\", StringType(), True),\n",
    "#                      ])\n",
    "\n",
    "# transactions_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/transactions/\")\n",
    "\n",
    "# # Create an empty DataFrame with the schema \n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.format(\"delta\").save(transactions_deltatable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51dbe18-509f-489c-9cd6-07789394c138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_commit_timestamp(deltatable_path):\n",
    "  # Load the Delta table\n",
    "  delta_table = DeltaTable.forPath(spark, deltatable_path)\n",
    "\n",
    "  # Get the history of the Delta table\n",
    "  history_df = delta_table.history()\n",
    "\n",
    "  # Get the timestamp of the last commit\n",
    "  last_commit_timestamp = history_df.orderBy(history_df['version'].desc()).select('timestamp').first()['timestamp']\n",
    "  return last_commit_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f90b4c8-f3bc-4320-86fb-bbfff56637d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_and_filter_files(last_commit_datetime, raw_path,expected_columns):\n",
    "  new_files = []\n",
    "  files = dbutils.fs.ls(raw_path)\n",
    "  for file in files:\n",
    "    if (datetime.fromtimestamp(file.modificationTime/1000))> last_commit_datetime:\n",
    "      df = spark.read.option(\"header\", \"true\").csv(file.path)\n",
    "      if set(df.columns) == set(expected_columns):\n",
    "        new_files.append(file.path)\n",
    "  return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b900a1dc-5a67-4557-b56e-f3e26bae47f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files found in the accounts\n"
     ]
    }
   ],
   "source": [
    "#accounts csv files transfer from raw to silver parquet\n",
    "\n",
    "expected_columns = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(accounts_deltatable_path)\n",
    "    .schema.names\n",
    ")\n",
    "last_commit_datetime = get_last_commit_timestamp(accounts_deltatable_path)\n",
    "filtered_files = list_and_filter_files(last_commit_datetime,accounts_raw_path, expected_columns)\n",
    "\n",
    "if filtered_files:\n",
    "  accounts_df = spark.read.option(\"header\", \"true\").csv(filtered_files)\n",
    "\n",
    "  # Remove duplicates \n",
    "  accounts_df = accounts_df.dropDuplicates()\n",
    "\n",
    "  #df = df.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "  accounts_df = accounts_df.withColumn(\"account_id\", col(\"account_id\").cast(\"integer\"))\n",
    "  accounts_df = accounts_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"integer\"))\n",
    "  accounts_df = accounts_df.withColumn(\"account_type\", col(\"account_type\").cast(\"string\"))\n",
    "  accounts_df = accounts_df.withColumn(\"balance\", col(\"balance\").cast(\"decimal(10,2)\"))\n",
    "\n",
    "  accounts_df.write.format(\"delta\").mode(\"append\").save(accounts_deltatable_path)\n",
    "\n",
    "else:\n",
    "  print(\"No new files found in the accounts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4546c82-077e-4720-b8bc-20ec10e1e2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files found in the accounts\n"
     ]
    }
   ],
   "source": [
    "#customer csv files transfer from raw to silver parquet\n",
    "\n",
    "expected_columns = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(customers_deltatable_path)\n",
    "    .schema.names\n",
    ")\n",
    "last_commit_datetime = get_last_commit_timestamp(customers_deltatable_path)\n",
    "filtered_files = list_and_filter_files(last_commit_datetime,customers_raw_path, expected_columns)\n",
    "\n",
    "if filtered_files:\n",
    "  customer_df = spark.read.option(\"header\", \"true\").csv(filtered_files)\n",
    "\n",
    "  # Remove duplicates \n",
    "  customer_df = customer_df.dropDuplicates()\n",
    "\n",
    "  #df = df.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "  customer_df = customer_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"integer\"))\n",
    "  customer_df = customer_df.withColumn(\"first_name\", col(\"first_name\").cast(\"string\"))\n",
    "  customer_df = customer_df.withColumn(\"last_name\", col(\"last_name\").cast(\"string\"))\n",
    "  customer_df = customer_df.withColumn(\"address\", col(\"address\").cast(\"string\"))\n",
    "  customer_df = customer_df.withColumn(\"city\", col(\"city\").cast(\"string\"))\n",
    "  customer_df = customer_df.withColumn(\"state\", col(\"state\").cast(\"string\"))\n",
    "  customer_df = customer_df.withColumn(\"zip\", col(\"zip\").cast(\"string\"))\n",
    "\n",
    "  customer_df.write.format(\"delta\").mode(\"append\").save(customers_deltatable_path)\n",
    "\n",
    "else:\n",
    "  print(\"No new files found in the customers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9617dc6a-f7de-42f0-884b-25977ad23bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files found in the loan payments\n"
     ]
    }
   ],
   "source": [
    "#loan payments csv files transfer from raw to silver parquet\n",
    "\n",
    "expected_columns = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(loan_payments_deltatable_path)\n",
    "    .schema.names\n",
    ")\n",
    "last_commit_datetime = get_last_commit_timestamp(loan_payments_deltatable_path)\n",
    "filtered_files = list_and_filter_files(last_commit_datetime,loan_payments_raw_path, expected_columns)\n",
    "\n",
    "if filtered_files:\n",
    "  loan_payments_df = spark.read.option(\"header\", \"true\").csv(filtered_files)\n",
    "\n",
    "  # Remove duplicates \n",
    "  loan_payments_df = loan_payments_df.dropDuplicates()\n",
    "\n",
    "  #df = df.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "  loan_payments_df = loan_payments_df.withColumn(\"payment_id\", col(\"payment_id\").cast(\"integer\"))\n",
    "  loan_payments_df = loan_payments_df.withColumn(\"loan_id\", col(\"loan_id\").cast(\"integer\"))\n",
    "  loan_payments_df = loan_payments_df.withColumn(\"payment_date\", col(\"payment_date\").cast(\"date\"))\n",
    "  loan_payments_df = loan_payments_df.withColumn(\"payment_amount\", col(\"payment_amount\").cast(\"integer\"))\n",
    "\n",
    "  loan_payments_df.write.format(\"delta\").mode(\"append\").save(loan_payments_deltatable_path)\n",
    "\n",
    "else:\n",
    "  print(\"No new files found in the loan payments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ff86b9-d2fd-4972-a102-588e9ff65348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files found in the loans\n"
     ]
    }
   ],
   "source": [
    "#loans csv files transfer from raw to silver parquet\n",
    "\n",
    "expected_columns = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(loans_deltatable_path)\n",
    "    .schema.names\n",
    ")\n",
    "last_commit_datetime = get_last_commit_timestamp(loans_deltatable_path)\n",
    "filtered_files = list_and_filter_files(last_commit_datetime,loans_raw_path, expected_columns)\n",
    "\n",
    "if filtered_files:\n",
    "  loans_df = spark.read.option(\"header\", \"true\").csv(filtered_files)\n",
    "\n",
    "  # Remove duplicates \n",
    "  loans_df = loans_df.dropDuplicates()\n",
    "\n",
    "  #df = df.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "  loans_df = loans_df.withColumn(\"loan_id\", col(\"loan_id\").cast(\"integer\"))\n",
    "  loans_df = loans_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"integer\"))\n",
    "  loans_df = loans_df.withColumn(\"loan_amount\", col(\"loan_amount\").cast(\"decimal(10, 2)\"))\n",
    "  loans_df = loans_df.withColumn(\"interest_rate\", col(\"interest_rate\").cast(\"decimal(4, 2)\"))\n",
    "  loans_df = loans_df.withColumn(\"loan_term\", col(\"loan_term\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "  loans_df.write.format(\"delta\").mode(\"append\").save(loans_deltatable_path)\n",
    "\n",
    "else:\n",
    "  print(\"No new files found in the loans\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1588c86-ca6a-4ac9-8c3b-81b33ce48bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files found in the loans\n"
     ]
    }
   ],
   "source": [
    "#transactions csv files transfer from raw to silver parquet\n",
    "\n",
    "expected_columns = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(transactions_deltatable_path)\n",
    "    .schema.names\n",
    ")\n",
    "last_commit_datetime = get_last_commit_timestamp(transactions_deltatable_path)\n",
    "filtered_files = list_and_filter_files(last_commit_datetime,transactions_raw_path, expected_columns)\n",
    "\n",
    "if filtered_files:\n",
    "  transactions_df = spark.read.option(\"header\", \"true\").csv(filtered_files)\n",
    "\n",
    "  # Remove duplicates \n",
    "  transactions_df = transactions_df.dropDuplicates()\n",
    "\n",
    "  #df = df.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "  transactions_df = transactions_df.withColumn(\"transaction_id\", col(\"transaction_id\").cast(\"integer\"))\n",
    "  transactions_df = transactions_df.withColumn(\"account_id\", col(\"account_id\").cast(\"integer\"))\n",
    "  transactions_df = transactions_df.withColumn(\"transaction_date\", col(\"transaction_date\").cast(\"date\"))\n",
    "  transactions_df = transactions_df.withColumn(\"transaction_amount\", col(\"transaction_amount\").cast(\"decimal(10, 2)\"))\n",
    "  transactions_df = transactions_df.withColumn(\"transaction_type\", col(\"transaction_type\").cast(\"string\"))\n",
    "\n",
    "\n",
    "  transactions_df.write.format(\"delta\").mode(\"append\").save(transactions_deltatable_path)\n",
    "\n",
    "else:\n",
    "  print(\"No new files found in the loans\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "raw to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}