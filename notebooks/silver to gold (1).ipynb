{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf987fb0-4581-4284-ae48-a805d73ad92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scope = 'cxkeyvault'\n",
    "sas_token_key = 'cxdatalake'\n",
    "storage_account = 'customeranalysisdatalake'\n",
    "container3 = 'gold'\n",
    "container2 = 'silver'\n",
    "\n",
    "sas_token = dbutils.secrets.get(scope=scope, key=sas_token_key)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, when, sum as _sum, first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd1ae11-b626-4254-bd08-b5c0080acb07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c218bd-0dfc-4e8f-9270-7fc70aa5f3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#all paths\n",
    "\n",
    "accounts_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/accounts/\")\n",
    "customers_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/customers/\")\n",
    "loan_payments_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/loan payments/\")\n",
    "loans_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/loans/\")\n",
    "transactions_deltatable_path = (f\"abfss://{container2}@{storage_account}.dfs.core.windows.net/transactions/\")\n",
    "\n",
    "total_balance_deltatable_path = (f\"abfss://{container3}@{storage_account}.dfs.core.windows.net/total balance/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46dcd154-dd5c-4bd7-b5d3-a5aacd7aec02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #total balance delta lake empty table\n",
    "\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType\n",
    "\n",
    "\n",
    "# schema = StructType([ StructField(\"customer_id\", IntegerType(), True),\n",
    "#                     StructField(\"first_name\", StringType(), True),\n",
    "#                     StructField(\"last_name\", StringType(), True),\n",
    "#                     StructField(\"address\", StringType(), True),\n",
    "#                     StructField(\"city\", StringType(), True),\n",
    "#                     StructField(\"state\", StringType(), True),\n",
    "#                     StructField(\"zip\", StringType(), True),\n",
    "#                     StructField(\"account_id\", IntegerType(), True),\n",
    "#                     StructField(\"account_type\", StringType(), True),\n",
    "#                     StructField(\"total_balance\", DecimalType(10,2), True)\n",
    "#                     ])\n",
    "\n",
    "# total_balance_deltatable_path = (f\"abfss://{container3}@{storage_account}.dfs.core.windows.net/total balance/\")\n",
    "\n",
    "# # Create an empty DataFrame with the schema \n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.format(\"delta\").save(total_balance_deltatable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067d2499-e916-4cce-8577-88372e9ad174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_commit_timestamp(deltatable_path):\n",
    "  # Load the Delta table\n",
    "  delta_table = DeltaTable.forPath(spark, deltatable_path)\n",
    "\n",
    "  # Get the history of the Delta table\n",
    "  history_df = delta_table.history()\n",
    "\n",
    "  # Get the timestamp of the last commit\n",
    "  last_commit_timestamp = history_df.orderBy(history_df['version'].desc()).select('timestamp').first()['timestamp']\n",
    "  return last_commit_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4793fc-d53c-4779-9b03-7c97ecb13661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no new data to append\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Delta tables\n",
    "accounts_df = spark.read.format(\"delta\").load(accounts_deltatable_path)\n",
    "customers_df = spark.read.format(\"delta\").load(customers_deltatable_path)\n",
    "total_balance_df = spark.read.format(\"delta\").load(total_balance_deltatable_path)\n",
    "\n",
    "# Join the accounts and customers DataFrames on customer_id\n",
    "joined_df = accounts_df.join(customers_df, \"customer_id\")\n",
    "\n",
    "# Group by customer_id and sum the balance\n",
    "result_df = joined_df.groupBy(\n",
    "    \"customer_id\", \n",
    "    \"first_name\", \n",
    "    \"last_name\", \n",
    "    \"address\", \n",
    "    \"city\", #\n",
    "    \"state\", \n",
    "    \"zip\"\n",
    ").agg(\n",
    "    _sum(\"balance\").alias(\"total_balance\"),\n",
    "    first(\"account_id\").alias(\"account_id\"),\n",
    "    first(\"account_type\").alias(\"account_type\")\n",
    ")\n",
    "\n",
    "# Select and include all columns\n",
    "final_df = result_df.select(\n",
    "    \"customer_id\",\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"address\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"zip\",\n",
    "    \"account_id\",\n",
    "    \"account_type\",\n",
    "    \"total_balance\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#balance column became decimal(20,2)\n",
    "final_df = final_df.withColumn(\"total_balance\", col(\"total_balance\").cast(\"decimal(10,2)\"))\n",
    "\n",
    "# Filter out rows that already exist in total_balance_df\n",
    "new_data_df = final_df.join(total_balance_df, [\"customer_id\"], \"left_anti\")\n",
    "\n",
    "# Append the new data to the total_balance Delta table\n",
    "new_data_df.write.format(\"delta\").mode(\"append\").save(total_balance_deltatable_path)\n",
    "\n",
    "#new_data_df.printSchema()\n",
    "#spark.read.format(\"delta\").load(total_balance_deltatable_path).printSchema()\n",
    "\n",
    "# Display the result\n",
    "if new_data_df.count() > 0:\n",
    "    display(new_data_df)\n",
    "else:\n",
    "    print(\"no new data to append\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver to gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}